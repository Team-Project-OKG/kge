dataset:
  files.entity_ids.filename: ../olpbench/entity_ids.del
  files.entity_ids.type: map
  files.relation_ids.filename: ../olpbench/relation_ids.del
  files.relation_ids.type: map
  files.entity_token_ids.filename: ../olpbench/entity_token_id_map.del
  files.entity_token_ids.type: map
  files.relation_token_ids.filename: ../olpbench/relation_token_id_map.del
  files.relation_token_ids.type: map
  files.entity_id_token_ids.filename: ../olpbench/bert-tiny_entity_id_tokens_ids_map.del
  files.entity_id_token_ids.type: sequence_map
  files.relation_id_token_ids.filename: ../olpbench/bert-tiny_relation_id_tokens_ids_map.del
  files.relation_id_token_ids.type: sequence_map

  name: olpbench_bert
#  num_entities: 2473409
#  num_relations: 961262
#  num_tokens_entities: 196007
#  num_tokens_relations: 39302
#  max_tokens_per_entity: 13
#  max_tokens_per_relation: 14

  files.train.filename: ../olpbench/train_thorough.del
  files.train.size: 30650783
  files.train.type: triples
  files.valid.filename: ../olpbench/validation_linked.del
  files.valid.size: 9959
  files.valid.type: quintuples
  files.test.filename: ../olpbench/test.del
  files.test.size: 9973
  files.test.type: quintuples


  #files.train.filename: train_basic.del
  #files.train.size: 31022420
  #files.train.type: triples
  #files.valid.filename: validation_all.del
  #files.valid.size: 9959
  #files.valid.type: triples

  pickle: True

  # Whether to use byte-pair encoding - only available for OLPDatasets. Split up
  # the tokens into sub-tokens and embed those.
  # Create separate sub-token vocabularies for token sequences of entities and
  # relations
  # 0 iterations_entities/relations: character-level sub-tokens. High memory during
  # training, very low preparation runtime.
  # Max iterations: If no more bigrams can be found -> token-level sub-tokens
  # lower memory during training, high preparation runtime
  byte_pair_encoding: False
  iterations_entities: 10
  iterations_relations: 10

  # list of all tokens that should be regarded as padding, i.e. receive an embedding of 0
  padding_indexes: [0]

  has_start_and_end_token: False
