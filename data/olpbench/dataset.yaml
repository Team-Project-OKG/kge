dataset:
  files.entity_ids.filename: entity_ids.del
  files.entity_ids.type: map
  files.relation_ids.filename: relation_ids.del
  files.relation_ids.type: map
  files.entity_token_ids.filename: entity_token_id_map.del
  files.entity_token_ids.type: map
  files.relation_token_ids.filename: relation_token_id_map.del
  files.relation_token_ids.type: map
  files.entity_id_token_ids.filename: entity_id_tokens_ids_map.del
  files.entity_id_token_ids.type: sequence_map
  files.relation_id_token_ids.filename: relation_id_tokens_ids_map.del
  files.relation_id_token_ids.type: sequence_map

  name: olpbench
#  num_entities: 2473409
#  num_relations: 961262
#  num_tokens_entities: 196007
#  num_tokens_relations: 39302
#  max_tokens_per_entity: 13
#  max_tokens_per_relation: 14

  files.train.filename: train_thorough.del
  files.train.size: 30650783
  files.train.type: triples
  files.valid.filename: validation_linked.del
  files.valid.size: 9959
  files.valid.type: quintuples
  files.test.filename: test.del
  files.test.size: 9973
  files.test.type: quintuples


  #files.train.filename: train_basic.del
  #files.train.size: 31022420
  #files.train.type: triples
  #files.valid.filename: validation_all.del
  #files.valid.size: 9959
  #files.valid.type: triples

  pickle: True
  # Whether to use byte-pair encoding - only available for OLPDatasets. Split up
  # the tokens into sub-tokens and embed those.
  # Create separate sub-token vocabularies for token sequences of entities and
  # relations
  # 0 iterations_entities/relations: character-level sub-tokens. Higher memory during
  # training, lower preparation time.
  # Max iterations: If no more bigrams can be found -> token-level sub-tokens
  # lower memory during training, higher preparation time
  #byte_pair_encoding: True
  #iterations_entities: 350
  #iterations_relations: 200

  # list of all tokens that should be regarded as padding, i.e. receive an embedding of 0
  padding_indexes: [0, 2, 3]

  has_start_and_end_token: True
