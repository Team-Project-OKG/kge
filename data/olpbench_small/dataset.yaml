dataset:
  files.entity_ids.filename: entity_id_map.del
  files.entity_ids.type: map
  files.relation_ids.filename: relation_id_map.del
  files.relation_ids.type: map
  files.entity_token_ids.filename: entity_token_id_map.del
  files.entity_token_ids.type: map
  files.relation_token_ids.filename: relation_token_id_map.del
  files.relation_token_ids.type: map
  files.entity_id_token_ids.filename: entity_id_tokens_ids_map.del
  files.entity_id_token_ids.type: sequence_map
  files.relation_id_token_ids.filename: relation_id_tokens_ids_map.del
  files.relation_id_token_ids.type: sequence_map

  name: olpbench-small
#  num_entities: 367
#  num_relations: 142
#  num_tokens_entities: 49
#  num_tokens_relations: 195
#  max_tokens_per_entity: 7
#  max_tokens_per_relation: 9

  files.test.filename: test_data.del
  files.test.size: 10
  files.test.type: quintuples
  files.train.filename: train_data_basic.del
  files.train.size: 126
  files.train.type: triples
  #files.valid.filename: validation_data_all.del
  files.valid.filename: validation_data_linked.del
  files.valid.size: 10
  files.valid.type: quintuples

  pickle: True
  # Whether to use byte-pair encoding - only available for OLPDatasets. Split up
  # the tokens into sub-tokens and embed those.
  # Create separate sub-token vocabularies for token sequences of entities and
  # relations
  # 0 iterations_entities/relations: character-level sub-tokens. Higher memory during
  # training, lower preparation time.
  # Max iterations: If no more bigrams can be found -> token-level sub-tokens
  # lower memory during training, higher preparation time
  #byte_pair_encoding: True
  #iterations_entities: 350
  #iterations_relations: 200

  # list of all tokens that should be regarded as padding, i.e. receive an embedding of 0
  padding_indexes: [0, 2, 3]

  has_start_and_end_token: True