# Hyperparameter search of with Wikipedia2vec default as word embeddings wothout fine tuning
import: [packed_lstm_lookup_embedder]

job.type: search
search.type: ax
valid.metric: mean_reciprocal_rank_filtered

ax_search:
  num_trials: 15
  num_sobol_trials: 15
  parameters:
    - name: train.optimizer.default.args.lr
      type: range
      bounds: [0.01, 0.2]
    - name: train.optimizer.default.args.weight_decay
      type: range
      bounds: [0.0000000001, 0.000001]
    - name: packed_lstm_lookup_embedder.dropout
      type: range
      bounds: [0.0, 0.1]


# settings for the dataset to use
dataset:
  type: olp
  name: olpbench
  byte_pair_encoding: False

# settings for ComplEx and the respective embedders
model: complex

complex:
  entity_embedder.type: packed_lstm_lookup_embedder
  relation_embedder.type: packed_lstm_lookup_embedder

packed_lstm_lookup_embedder:
  initialize: xavier_uniform_
  pretrained:
    use: True
    file:
      name: enwiki_20180420_500d
      type: txt
    oov_tactic: 'random'
    freeze: False
    use_pickle: True

# settings for the training job of each hyperparameter combination
train:
  type: negative_sampling
  max_epochs: 10
  batch_size: 4096
  subbatch_auto_tune: True
  optimizer: Adagrad


negative_sampling:
  shared: True
  with_replacement: False
  implementation: batch
  samples_within_batch: True


# settings for the evaluation job
eval:
  type: olp_entity_ranking
  batch_size: 500
