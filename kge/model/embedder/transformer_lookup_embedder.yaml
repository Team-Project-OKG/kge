# Stores explicitly an embedding for each object in a lookup table. See
# https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding
transformer_lookup_embedder:
  class_name: TransformerLookupEmbedder

  # parameters specific to Transformer Lookup Embedder
  nhead: 8
  num_layers: 3
  pooling: mean
  dim_ff: 2048
  factor: 1
  warmup: 4000
  custom_lr: True
  transformer_dropout: 0.1
  # general settings for mention embedders (see mention_embedder.yaml for explanations)
  requires_start_and_end_token: False

  cut_padding_in_batch: True
  set_padding_embeddings_to_0: True

  bin_within_batch: False
  bin_size: 250

  token_embedding_model:
    use: False
    name: distilbert
    precache: 0

  pretrained:
    use: False
    file:
      name: glove.840B.300d_word2vec
      type: txt
    oov_tactic: 'random'
    freeze: False
    use_pickle: True

  # settings for lookup embedders (see lookup_embedder.yaml for explanations)
  dim: 512

  initialize: normal_
  initialize_args:
    +++: +++
  pretrain:
      model_filename: ""
      ensure_all: False
  dropout: 0.
  normalize:
    p: -1.
  regularize: 'lp'
  regularize_weight: 0.0
  regularize_args:
    weighted: False
    p: 2
    +++: +++
  sparse: False
  round_dim_to: []