transformer_lookup_embedder:
  class_name: TransformerLookupEmbedder

  # parameters specific to Transformer Lookup Embedder

  #number of attention heads per self-attention layer
  nhead: 8

  #number of encoder layers
  num_layers: 3

  #pooling method (choice of mean, cls, sum and max)
  pooling: mean

  #dimension of the feed forward layer of each encoder layer
  dim_ff: 256

  #dropout of the encoder layers as well as the positional embeddings
  transformer_dropout: 0.1


  # general settings for mention embedders (see mention_embedder.yaml for explanations)
  requires_start_and_end_token: False

  cut_padding_in_batch: True
  set_padding_embeddings_to_0: True

  bin_within_batch: False
  bin_size: 250

  token_embedding_model:
    use: False
    name: distilbert
    precache: 0
    freeze: False

  pretrained:
    use: False
    file:
      name: glove.840B.300d_word2vec
      type: txt
    oov_tactic: 'random'
    freeze: False
    use_pickle: True

  # settings for lookup embedders (see lookup_embedder.yaml for explanations)
  dim: 512

  initialize: normal_
  initialize_args:
    +++: +++
  pretrain:
      model_filename: ""
      ensure_all: False
  dropout: 0.
  normalize:
    p: -1.
  regularize: 'lp'
  regularize_weight: 0.0
  regularize_args:
    weighted: False
    p: 2
    +++: +++
  sparse: False
  round_dim_to: []