# Stores explicitly an embedding for each object in a lookup table. See
# https://pytorch.org/docs/stable/nn.html#torch.nn.Embedding
bigram_lookup_embedder:
  class_name: BigramLookupEmbedder

  # parameters specific to Bigram Lookup Embedder

  # which pooling algorithm to use ("max", "sum", "mean")
  pooling: "mean"

  # general settings for mention embedders (see mention_embedder.yaml for explanations)
  requires_start_and_end_token: True

  cut_padding_in_batch: True
  set_padding_embeddings_to_0: True

  bin_within_batch: False
  bin_size: 0

  token_embedding_model:
    use: False
    name: distilbert
    precache: 0

  pretrained:
    use: False
    file:
      name: glove.840B.300d_word2vec
      type: txt
    oov_tactic: 'random'
    freeze: False
    use_pickle: True

  # settings for lookup embedders (see lookup_embedder.yaml for explanations)
  dim: 100
  initialize: normal_
  initialize_args:
    +++: +++
  pretrain:
      model_filename: ""
      ensure_all: False
  dropout: 0.
  normalize:
    p: -1.
  regularize: 'lp'
  regularize_weight: 0.0
  regularize_args:
    weighted: False
    p: 2
    +++: +++
  sparse: False
  round_dim_to: []
