# Settings that must be specified for any embedder inheriting from mention embedder
# Note: This yaml itself is never used, but rather provides the list of parameters and explanations.

any_mention_embedder:

  # Whether the embedder requires a start and end token as padding, e.g. False for LSTM and True for Bigram based on
  # CNN. The tokens must be present in the token mapping of the dataset, and should be specified as padding indexes in
  # the yaml of the dataset.
  requires_start_and_end_token: False

  # Whether to cut as much padding as possible within the batch while still remaining the tensor structure. E.g. max
  # token length in the dataset is 10, but only 8 within a given batch. Then, the last two paddings will be removed
  # in the given batch.
  cut_padding_in_batch: True
  # Whether to ensure that the embeddings of the padding indexes specified in the dataset.yaml are 0.
  set_padding_embeddings_to_0: True

  # To speed up embedding and training, a mention embedder can partition the data within one batch based on sequence
  # length. Padding is removed for each bin as much as possible.
  bin_within_batch: False
  # The minimum size of each bin. 0 will create a bin for each length and thus remove all padding, but decrease
  # performance due to decreased parallelization.
  bin_size: 0

  # Token embedding models are used to get embeddings instead of using torch.nn.Embeddings. This freezes embeddings,
  # i.e. they are not adjusted during training.
  token_embedding_model:
    # Enables or disables token embedding models
    use: False
    # Uses models from https://huggingface.co/models . Implemented to be able to use bert, distilbert and albert models.
    # Needs corresponding tokens that can be created by using the same model in utils.create_bert_tokens
    name: distilbert
    # Freeze the token embedding model to reduce ressource consumption if needed.
    freeze: False
    # Sets the number of entities that should be cached. Caches most frequent entities, decreases runtime significantly.
    # Good number for OLPBench is 15000, which cover roughly half of the occurences in training.
    # Only possible if freeze is True.
    precache: 0


  # Parameters related to using pre-trained word embeddings.
  pretrained:
    # Flag or whether to use pre-trained word embeddings or not.
    use: False
    # Information about the file to receive pre-trained embeddings from. Must be in word2vec format.
    file:
      # file name (without file type)
      name: glove.840B.300d_word2vec
      # file type
      type: txt
    # How to deal with out of vocabulary errors:
    #  - random: Use random embedding initialized based on parameter "initialize" (inherited from Lookup Embedder)
    oov_tactic: 'random'
    # Flag whether to not change embeddings during training
    freeze: False
    # Save / load dataset specific tensor of token embeddings based on the given file. The pickle can be used as a
    # starting point for further training jobs, speeding up initialization of a mention embedder.
    use_pickle: True