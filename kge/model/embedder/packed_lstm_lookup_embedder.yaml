packed_lstm_lookup_embedder:
  class_name: PackedLstmLookupEmbedder

  # parameters specific to Packed LSTM Lookup Embedder

  # Flag whether to use the dimensionality of embeddings as the dimensionality of the hidden (and output) state of the
  # LSTM. If false, set dimensionality of hidden state with "hidden_dim".
  emb_dim_as_hidden_dim: True
  hidden_dim: 0

  # Number of layers for LSTM. Default is 1 - no stacking
  num_layers: 1

  # general settings for mention embedders (see mention_embedder.yaml for explanations)
  requires_start_and_end_token: False

  cut_padding_in_batch: True
  set_padding_embeddings_to_0: True

  bin_within_batch: False
  bin_size: 0

  token_embedding_model:
    use: False
    name: distilbert
    precache: 0
    freeze: False

  pretrained:
    use: False
    file:
      name: glove.840B.300d_word2vec
      type: txt
    oov_tactic: 'random'
    freeze: False
    use_pickle: True

  # settings for lookup embedders (see lookup_embedder.yaml for explanations)
  dim: 100
  initialize: normal_
  initialize_args:
    +++: +++
  pretrain:
      model_filename: ""
      ensure_all: False
  dropout: 0.
  normalize:
    p: -1.
  regularize: 'lp'
  regularize_weight: 0.0
  regularize_args:
    weighted: False
    p: 2
    +++: +++
  sparse: False
  round_dim_to: []
