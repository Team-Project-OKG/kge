import: [transformer_lookup_embedder]                                         #!!! adjust if model is changed


# settings for the dataset to use
dataset:
  type: olp
  name: olpbench_small

# settings for ComplEx and the respective embedders
model: complex

complex:
  entity_embedder.type: transformer_lookup_embedder                          #!!! adjust if model is changed
  relation_embedder.type: transformer_lookup_embedder                        #!!! adjust if model is changed

# Best results from 20210123-212939-olpbench_transformer_random_search/00006
transformer_lookup_embedder:                                                  #!!! adjust if model is changed (incl. parameters of the model)
  initialize: xavier_uniform_ # is Glorot initialization
  dim: 512
  dropout: 0.0 #0.06814730241894722
  factor: 1
  warmup: 4000
  custom_lr: True
# settings for the training job of each hyperparameter combination
train:
  type: negative_sampling
  max_epochs: 1000
  batch_size: 10
  subbatch_auto_tune: True

  #lr_scheduler: Transformer_Scheduler
  optimizer:
    +++: +++
    default:
      args:
        +++: +++
        eps: 1e-9
        lr: 0.00004
        weight_decay: 3.4979861306445674e-07
      type: Adam



# settings for negative sampling
# use number of replacements equal to the batch size to approximate Samuel's negative sampling algorithm
# num_samples describes the length of individual samples, not the number of samples. e.g. num_sample: 4 -> [rand, rand, rand, rand]; num_sample:3 -> [rand, rand, rand]
#!!! look into how to reproduce Samuel's negative sampling algorithm
negative_sampling:
  shared: True
  with_replacement: False
  implementation: batch
  samples_within_batch: True
  num_samples:
    s: 1
    p: 0          # -1 means: same as s
    o: -1

valid:
  every: 10
  early_stopping:
    patience: 10

# settings for the evaluation job
eval:
  type: olp_entity_ranking
  batch_size: 10
